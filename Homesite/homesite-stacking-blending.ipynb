{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homesite Quote Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which customers will purchase a quoted insurance plan? [Kaggle - Homesite Quote Conversion](https://www.kaggle.com/c/homesite-quote-conversion)\n",
    "\n",
    "Before asking someone on a date or skydiving, it's important to know your likelihood of success. The same goes for quoting home insurance prices to a potential customer. [Homesite](https://homesite.com/), a leading provider of homeowners insurance, does not currently have a dynamic conversion rate model that can give them confidence a quoted price will lead to a purchase. \n",
    "\n",
    "Using an anonymized database of information on customer and sales activity, including property and coverage information, Homesite is challenging you to predict which customers will purchase a given quote. Accurately predicting conversion would help Homesite better understand the impact of proposed pricing changes and maintain an ideal portfolio of customer segments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking & Blending Different Classifiers - Creating a Meta Classifier with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I learnt this technique from [Kaggle Ensembling Guide](http://mlwave.com/kaggle-ensembling-guide/) and from this [Kaggle post]( http://www.kaggle.com/c/bioresponse/forums/t/1889/question-about-the-process-of-ensemble-learning/10950#post10950)\n",
    "\n",
    "**Explanation:** In each of the cross validation folds, train the models, then stack the validation test results for each classifier to form one 'meta-variable' column. So, for each classifier we will have a separate column. This constitutes our training data for the next stage LogisticRegression meta-classifier. At the same time during each cross validation fold training, use the learned sub model in that fold of each classifier to predict on the entire test data. At the end of all the cross validation rounds take average prediction of the test data for each classifier across each folds. Stack the averaged test predictions side by side columnwise, this will act as the test data for the next stage LogisticRegression meta-classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Loading Feature Extracted Cleaned Data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/feature_extracted_train.csv')\n",
    "test_df = pd.read_csv('data/feature_extracted_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define training and testing sets\n",
    "train_label = train_df['QuoteConversion_Flag']\n",
    "trainset = train_df.drop('QuoteConversion_Flag', axis=1)\n",
    "testset = test_df.copy()\n",
    "testset = testset[X_train.columns.tolist()] # maintain same column order between train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_folds = 4\n",
    "n_threads = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create a Blend of Classifiers **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Level 0 classifiers\n",
    "clfs = [RandomForestClassifier(n_estimators=100, n_jobs=n_threads, criterion='gini'),\n",
    "        RandomForestClassifier(n_estimators=100, n_jobs=n_threads, criterion='entropy'),\n",
    "        ExtraTreesClassifier(n_estimators=100, n_jobs=n_threads, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=100, n_jobs=n_threads, criterion='entropy'),\n",
    "        GradientBoostingClassifier(n_estimators=50, learning_rate=0.05, subsample=0.5, max_depth=6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stratified random shuffled cross validation\n",
    "skf = list(StratifiedKFold(y_train, n_folds, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create train and test sets for blending and Pre-allocate the data\n",
    "blend_train = np.zeros((X_train.shape[0], len(clfs)))\n",
    "blend_test = np.zeros((X_train.shape[0], len(clfs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Stack Predictions of Classifiers **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each classifier, we train the number of fold times (=len(skf))\n",
    "for clf_index, clf in enumerate(clfs):\n",
    "    print('Training classifier [%s]' % (clf_index + 1))\n",
    "    print(clf)\n",
    "\n",
    "    blend_test_j = np.zeros((testset.shape[0], len(skf)))  # Number of testing data x Number of folds , we will take the mean of the predictions\n",
    "\n",
    "    for fold_index, (train_index, valid_index) in enumerate(skf):\n",
    "        print('Fold [%s]' % (fold_index + 1))\n",
    "\n",
    "        # Cross validation training and validation set\n",
    "        X_train = trainset.iloc[train_index]\n",
    "        y_train = train_label.iloc[train_index]\n",
    "        X_valid = trainset.iloc[valid_index]\n",
    "        y_valid = train_label.iloc[valid_index]\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # This output will be the basis for our blended classifier to train against,\n",
    "        # which is also the output of our classifiers\n",
    "        blend_train[valid_index, clf_index] = clf.predict_proba(X_valid)[:, 1]\n",
    "        blend_test_j[:, fold_index] = clf.predict_proba(testset)[:, 1]\n",
    "\n",
    "    # Take the mean of the predictions of the cross validation set. Each column is now a meta-feature\n",
    "    blend_test[:, clf_index] = blend_test_j.mean(axis=1)\n",
    "\n",
    "    # Another way of doing this instead of predicting on the cv set, the level 0 estimator can be trained on the full data again\n",
    "    # and take the prediction on the full testset\n",
    "    #clf.fit(trainset, train_label)\n",
    "    #blend_test[:, clf_index] = clf.predict_proba(testset)\n",
    "\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Blend Predictions of Classifiers using Logistic Regression **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Blending using LogisticRegression')\n",
    "bclf = LogisticRegression()\n",
    "bclf.fit(blend_train, train_label)\n",
    "y_pred_proba = bclf.predict_proba(blend_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Linear stretch of predictions to [0,1]')\n",
    "y_pred_proba = (y_pred_proba - y_pred_proba.min()) / (y_pred_proba.max() - y_pred_proba.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Writing Final Submission File')\n",
    "preds_out = pd.read_csv('data/sample_submission.csv')\n",
    "preds_out['QuoteConversion_Flag'] = y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preds_out = preds_out.set_index('QuoteNumber')\n",
    "preds_out.to_csv('homesite_blended_RF_ET_GBM_with_FE_nan.csv', index=False, float_format='%0.9f')\n",
    "print 'Done'"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

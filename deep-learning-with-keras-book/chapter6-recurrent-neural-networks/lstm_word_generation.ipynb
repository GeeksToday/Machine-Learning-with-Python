{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Recurrent Neural Network\n",
    "\n",
    "This notebook is a replica of Chapter 6: Recurrent Neural Network from the Deep Learning with Keras book by Antonio Gulli, Sujit Paul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs have been used extensively by the **natural language processing (NLP)** community for various applications. One such application is building language models. A language model allows us to predict the probability of a word in a text given the previous words. Language models are important for various higher level tasks such as machine translation, spelling correction, and so on.\n",
    "\n",
    "A side effect of the ability to predict the next word given previous words is a generative model that allows us to generate text by sampling from the output probabilities. In language modeling, our input is typically a sequence of words and the output is a sequence of predicted words. The training data used is existing unlabeled text, where we set the label $y_t$ at time $t$ to be the input $x_{t+1}$ at time $t+1$.\n",
    "\n",
    "In this example of using Keras for building LSTMs, we will train a character based language model on the text of *Alice in Wonderland* to predict the next character given 10 previous characters. We have chosen to build a character-based model here because it has a smaller vocabulary and trains quicker. The idea is the same as using a word-based language model, except we use characters instead of words. We will then use the trained model to generate some text in the same style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import collections\n",
    "\n",
    "import re, nltk\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "\n",
    "# this allows plots to appear directly in the notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/gpu:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read our input text from the text of Alice in Wonderland on the Project Gutenberg website (http://www.gutenberg.org/files/11/11-0.txt). The file contains line breaks and non- ASCII characters, so we do some preliminary cleanup and write out the contents into a variable called `text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_FILE = \"data/moes_tavern_lines.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = open(INPUT_FILE).read().lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(set(text))\n",
    "nb_words = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of unique words 10353\n"
     ]
    }
   ],
   "source": [
    "print(\"total number of unique words\", nb_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are building a character-level LSTM, our vocabulary is the set of characters that occur in the text. There are 60 of them in our case. Since we will be dealing with the indexes to these characters rather than the characters themselves, the following code snippet creates the necessary lookup tables:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Here chars is the number of features in our character \"vocabulary\"\n",
    "chars = sorted(list(set(text)))\n",
    "nb_chars = len(chars)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(chars)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('Total vocab:', nb_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating lookup tables i.e. mapping of unique chars to integers\n",
    "word2index = dict((c, i) for i, c in enumerate(words))\n",
    "index2word = dict((i, c) for i, c in enumerate(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create the input and label texts. We do this by stepping through the text by a number of characters given by the `STEP` variable (`1` in our case) and then extracting a span of text whose size is determined by the `SEQLEN` variable (`10` in our case). The next character after the span is our label character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create inputs and labels from the text. We do this by stepping\n",
    "# through the text ${step} character at a time, and extracting a \n",
    "# sequence of size ${seqlen} and the next output char. For example,\n",
    "# assuming an input text \"The sky was falling\", we would get the \n",
    "# following sequence of input_chars and label_chars (first 5 only)\n",
    "#   The sky wa -> s\n",
    "#   he sky was ->  \n",
    "#   e sky was  -> f\n",
    "#    sky was f -> a\n",
    "#   sky was fa -> l\n",
    "SEQLEN = 20\n",
    "STEP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_words = []\n",
    "target_words = []\n",
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    input_words.append(text[i:i + SEQLEN])\n",
    "    target_words.append(text[i + SEQLEN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to vectorize these input and label texts. Each row of the input to the LSTM corresponds to one of the input texts shown previously. There are `SEQLEN` characters in this input, and since our vocabulary size is given by `nb_chars`, we represent each input character as a one-hot encoded vector of size (`nb_chars`). Thus each input row is a tensor of size (`SEQLEN` and `nb_chars`). Our output label is a single character, so similar to the way we represent each character of our input, it is represented as a one-hot vector of size (`nb_chars`). Thus, the shape of each label is `nb_chars`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X shape to be [samples, time steps, features]\n",
    "# Y shape to be [samples, features]\n",
    "X = np.zeros((len(input_words), SEQLEN, nb_words), dtype=np.bool)\n",
    "y = np.zeros((len(input_words), nb_words), dtype=np.bool)\n",
    "for i, input_word in enumerate(input_words):\n",
    "    for j, word in enumerate(input_word):\n",
    "        X[i, j, word2index[word]] = 1\n",
    "    y[i, word2index[target_words[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to build our model. We define the LSTM's output dimension to have a size of 128. This is a hyper-parameter that needs to be determined by experimentation. In general, if we choose too small a size, then the model does not have sufficient capacity for generating good text, and you will see long runs of repeating characters or runs of repeating word groups. On the other hand, if the value chosen is too large, the model has too many parameters and needs a lot more data to train effectively. We want to return a single character as output, not a sequence of characters, so `return_sequences=False`. We have already seen that the input to the LSTM is of shape (`SEQLEN` and `nb_chars`). In addition, we set `unroll=True` because it improves performance on the TensorFlow backend.\n",
    "\n",
    "The LSTM is connected to a dense (fully connected) layer. The dense layer has (`nb_char`) units, which emits scores for each of the characters in the vocabulary. The activation on the dense layer is a `softmax`, which normalizes the scores to probabilities. The character with the highest probability is chosen as the prediction. We compile the model with the categorical cross-entropy loss function, a good loss function for categorical outputs, and the RMSprop optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the model. We use a single RNN with a fully connected layer\n",
    "# to compute the most likely predicted output char\n",
    "HIDDEN_SIZE = 256\n",
    "BATCH_SIZE = 128\n",
    "NUM_ITERATIONS = 15\n",
    "NUM_PREDS_PER_EPOCH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=HIDDEN_SIZE, return_sequences=True, input_shape=(SEQLEN, nb_words), unroll=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=HIDDEN_SIZE, return_sequences=False, unroll=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(nb_words))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 20, 256)           10864640  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10353)             2660721   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10353)             0         \n",
      "=================================================================\n",
      "Total params: 14,050,673\n",
      "Trainable params: 14,050,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training approach is a little different from what we have seen so far. So far our approach has been to train a model for a fixed number of epochs, then evaluate it against a portion of held-out test data. Since we don't have any labeled data here, we train the model for an epoch (`NUM_EPOCHS_PER_ITERATION=1`) then test it. We continue training like this for 25 (`NUM_ITERATIONS=25`) iterations, stopping once we see intelligible output. So effectively, we are training for `NUM_ITERATIONS` epochs and testing the model after each epoch.\n",
    "\n",
    "Our test consists of generating a character from the model given a random input, then dropping the first character from the input and appending the predicted character from our previous run, and generating another character from the model. We continue this 100 times (`NUM_PREDS_PER_EPOCH=100`) and generate and print the resulting string. The string gives us an indication of the quality of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Iteration #: 0\n",
      "Epoch 1/1\n",
      "48966/48966 [==============================] - 421s 9ms/step - loss: 7.4106\n",
      "Generating from seed: car. homer_simpson: what about the cops? moe_szyslak: that's the beauty part. every cop in town's gonna be on the police\n",
      "car. homer_simpson: what about the cops? moe_szyslak: that's the beauty part. every cop in town's gonna be on the police the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the ==================================================\n",
      "Iteration #: 1\n",
      "Epoch 1/1\n",
      "48966/48966 [==============================] - 415s 8ms/step - loss: 7.2395\n",
      "Generating from seed: they're on top of the world while i'm sittin' here pretending i have a stool. homer_simpson: (sour) all because of\n",
      "they're on top of the world while i'm sittin' here pretending i have a stool. homer_simpson: (sour) all because of the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the ==================================================\n",
      "Iteration #: 2\n",
      "Epoch 1/1\n",
      "48966/48966 [==============================] - 412s 8ms/step - loss: 7.1790\n",
      "Generating from seed: had thirty thousand here last night. now play. the audience is gettin' restless. barney_gumble: we want chilly willy! we want\n",
      "had thirty thousand here last night. now play. the audience is gettin' restless. barney_gumble: we want chilly willy! we want the moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: ==================================================\n",
      "Iteration #: 3\n",
      "Epoch 1/1\n",
      "48966/48966 [==============================] - 412s 8ms/step - loss: 7.3763\n",
      "Generating from seed: fortress of vengeance.\" oh, i am so there! professor_jonathan_frink: we studied traffic patterns and found that drivers move the fastest\n",
      "fortress of vengeance.\" oh, i am so there! professor_jonathan_frink: we studied traffic patterns and found that drivers move the fastest moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: moe_szyslak: ==================================================\n",
      "Iteration #: 4\n",
      "Epoch 1/1\n",
      "48966/48966 [==============================] - 412s 8ms/step - loss: 7.0576\n",
      "Generating from seed: angry. carl_carlson: and i'm magnanimous in victory. moe_szyslak: wow, that's the best book i've ever seen! homer_simpson: (checking book) no,\n",
      "angry. carl_carlson: and i'm magnanimous in victory. moe_szyslak: wow, that's the best book i've ever seen! homer_simpson: (checking book) no, i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i ==================================================\n",
      "Iteration #: 5\n",
      "Epoch 1/1\n",
      "48966/48966 [==============================] - 412s 8ms/step - loss: 6.9834\n",
      "Generating from seed: new barney? moe_szyslak: hey, every bar needs a world-class drunk. lenny_leonard: yeah, someone who makes our alcoholism seem less raging.\n",
      "new barney? moe_szyslak: hey, every bar needs a world-class drunk. lenny_leonard: yeah, someone who makes our alcoholism seem less raging. the the new the the little little little the the the the the the the the the the the the the the little moe_szyslak: i i i i i i i i i i i i i i i i i i i i i i i i i i ==================================================\n",
      "Iteration #: 6\n",
      "Epoch 1/1\n",
      "48966/48966 [==============================] - 412s 8ms/step - loss: 6.8882\n",
      "Generating from seed: you need a little more \"milk of amnesia.\" moe_szyslak: dang, i'm missing the secret ingredient! uh, lemme squeeze some more\n",
      "you need a little more \"milk of amnesia.\" moe_szyslak: dang, i'm missing the secret ingredient! uh, lemme squeeze some more new new new new new new new new new new moe_szyslak: moe_szyslak: i i i i i i i got the lot moe_szyslak: new new new new new new new new new new new moe_szyslak: moe_szyslak: i i i have a new new new new new new new new new ==================================================\n",
      "Iteration #: 7\n",
      "Epoch 1/1\n",
      "48966/48966 [==============================] - 412s 8ms/step - loss: 6.7811\n",
      "Generating from seed: you better be dyin'! homer_simpson: well, that's it. i guess this is the class i'm gonna die in. moe_szyslak: eh,\n",
      "you better be dyin'! homer_simpson: well, that's it. i guess this is the class i'm gonna die in. moe_szyslak: eh, i got the new of the new of the new new of the new is the new is the new is moe_szyslak: moe_szyslak: i got a new of the new new new new new new new new new is moe_szyslak: moe_szyslak: i got a little new is moe_szyslak: moe_szyslak: bar ==================================================\n",
      "Iteration #: 8\n",
      "Epoch 1/1\n",
      "48966/48966 [==============================] - 411s 8ms/step - loss: 6.7294\n",
      "Generating from seed: you know, i lost my ma at christmas. moe_szyslak: she took me to a mall and i never saw her\n",
      "you know, i lost my ma at christmas. moe_szyslak: she took me to a mall and i never saw her little little new little new is the new new bar moe_szyslak: i got the new new bar moe_szyslak: i got the new bar moe_szyslak: i can't don't don't don't got the new new new bar moe_szyslak: i got the new bar moe_szyslak: i got the new of the new new ==================================================\n",
      "Iteration #: 9\n",
      "Epoch 1/1\n",
      "48966/48966 [==============================] - 26368s 539ms/step - loss: 6.7361\n",
      "Generating from seed: she made this tape that i think you should hear. lisa_simpson: (on tape) dear moe, if anything should ever happen\n",
      "she made this tape that i think you should hear. lisa_simpson: (on tape) dear moe, if anything should ever happen moe_szyslak: hey, i know you was one moe_szyslak: and i think i got a little little little little little little is the little is in the new little little little little little little little is you just just just the little little little little little little is the little is ==================================================\n",
      "Iteration #: 10\n",
      "Epoch 1/1\n",
      "48966/48966 [==============================] - 422s 9ms/step - loss: 6.7738\n",
      "Generating from seed: die. they look pretty comfortable. homer_simpson: (morose) yeah, i guess. carl_carlson: and, uh, are those your original lips? homer_simpson: well,\n",
      "die. they look pretty comfortable. homer_simpson: (morose) yeah, i guess. carl_carlson: and, uh, are those your original lips? homer_simpson: well, i need the guy i think a bar is the bar homer_simpson: little little bar is the bar moe_szyslak: yeah, i need you be little little guy moe_szyslak: yeah, i got the little guy moe_szyslak: yeah, i got the bar of the bar homer_simpson: yeah, i think is the little ==================================================\n",
      "Iteration #: 11\n",
      "Epoch 1/1\n",
      "48966/48966 [==============================] - 419s 9ms/step - loss: 6.9411\n",
      "Generating from seed: fun as beer. sure, i'm all dizzy and nauseous, but where's the inflated sense of self-esteem? male_inspector: (shocked) man alive!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun as beer. sure, i'm all dizzy and nauseous, but where's the inflated sense of self-esteem? male_inspector: (shocked) man alive! homer_simpson: i sure let me won't sure to little little a a a little need of the little little thing i got a thing in the a little to could could be to little love moe_szyslak: little little need homer_simpson: i got me to be to little little little need ==================================================\n",
      "Iteration #: 12\n",
      "Epoch 1/1\n",
      "48966/48966 [==============================] - 419s 9ms/step - loss: 6.9553\n",
      "Generating from seed: thank god there's no alcohol in this bar or this place would really go up. homer_simpson: (gasps) what happened here?\n",
      "thank god there's no alcohol in this bar or this place would really go up. homer_simpson: (gasps) what happened here? moe_szyslak: hey, look time you to the little to be a little thing i want to get a little to were good to be a little to time you be one of a to time homer_simpson: more is the to little thing i got a little to a little to ==================================================\n",
      "Iteration #: 13\n",
      "Epoch 1/1\n",
      "48966/48966 [==============================] - 423s 9ms/step - loss: 7.1035\n",
      "Generating from seed: of college! (\"oh yeah\") it's too sad! kent_brockman: tomorrow's forecast is: moe_szyslak: okay, place your bets. lenny_leonard: partly cloudy! carl_carlson:\n",
      "of college! (\"oh yeah\") it's too sad! kent_brockman: tomorrow's forecast is: moe_szyslak: okay, place your bets. lenny_leonard: partly cloudy! carl_carlson: (from bar is bar thing moe_szyslak: bar tv) bar is good tv) i need good tv) tv) tv) yeah, let you had your new new little new to be thing thing i'm gonna to thing moe_szyslak: moe_szyslak: tv) hey, let it's thing and we had me a new to thing ==================================================\n",
      "Iteration #: 14\n",
      "Epoch 1/1\n",
      "48966/48966 [==============================] - 423s 9ms/step - loss: 6.8129\n",
      "Generating from seed: oh yeah. moe_szyslak: i made that last one up. homer_simpson: (keeping dignity) i see. moe_szyslak: sounds like you're having a\n",
      "oh yeah. moe_szyslak: i made that last one up. homer_simpson: (keeping dignity) i see. moe_szyslak: sounds like you're having a new thing you get be thing i need it was the thing and the new thing and one thing think and i got that is one guy be thing for the thing thing the thing don't get back of want and bar to be thing and much good more thing ==================================================\n",
      "Iteration #: 15\n",
      "Epoch 1/1\n",
      "48966/48966 [==============================] - 425s 9ms/step - loss: 7.2826\n",
      "Generating from seed: week, huh? in an unrelated matter, (seductive) i just put on cologne and shaved my knuckles. marge_simpson: i'm still hoping\n",
      "week, huh? in an unrelated matter, (seductive) i just put on cologne and shaved my knuckles. marge_simpson: i'm still hoping to time big love of a to could got out of me. homer_simpson: man, hey, moe, got not get out of the a a thing could love my thing bart_simpson: could time out more to be time i need that think could could want for be want for a place ==================================================\n",
      "Iteration #: 16\n",
      "Epoch 1/1\n",
      "48966/48966 [==============================] - 423s 9ms/step - loss: 6.9816\n",
      "Generating from seed: up coaster) this coaster's fine. moe_szyslak: you are not my friends. to me you're just mouths drinkin' beers! lenny_leonard: you\n",
      "up coaster) this coaster's fine. moe_szyslak: you are not my friends. to me you're just mouths drinkin' beers! lenny_leonard: you can't say you moe_szyslak: hey i think i could be say in my bar -- and then he mean that bar will be much more of my bar is -- lenny_leonard: yeah, moe, i can have me in my bar -- and -- then she mean -- moe_szyslak: would did ==================================================\n",
      "Iteration #: 17\n",
      "Epoch 1/1\n",
      "48966/48966 [==============================] - 417s 9ms/step - loss: 6.8559\n",
      "Generating from seed: contest here... (intrigued) or do i? moe_szyslak: ... and if anybody wants potato chips or anything fancy, tell 'em to\n",
      "contest here... (intrigued) or do i? moe_szyslak: ... and if anybody wants potato chips or anything fancy, tell 'em to get my you was need is get a one love homer_simpson: i'm said i was not is the love for my bar can can can be my little love carl_carlson: (to said i'm love a bar need i said i gotta be love that money. homer_simpson: don't gotta be you ==================================================\n",
      "Iteration #: 18\n",
      "Epoch 1/1\n",
      "36736/48966 [=====================>........] - ETA: 1:44 - loss: 6.9988"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-6f6e781bc0fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Iteration #: %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# testing model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Work\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mD:\\Work\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1657\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mD:\\Work\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Work\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2357\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Work\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Work\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Work\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mD:\\Work\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Work\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We train the model in batches and test output generated at each step\n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Iteration #: %d\" % (iteration))\n",
    "    \n",
    "    model.fit(X, y, epochs=1)\n",
    "    \n",
    "    # testing model\n",
    "    # randomly choose a row from input_chars, then use it to \n",
    "    # generate text from model for next 100 chars\n",
    "    test_idx = np.random.randint(len(input_words))\n",
    "    test_words = input_words[test_idx]\n",
    "    print(\"Generating from seed: %s\" % (\" \".join(test_words)))\n",
    "    print(\" \".join(test_words), end=\" \")\n",
    "    for i in range(NUM_PREDS_PER_EPOCH):\n",
    "        X_test = np.zeros((1, SEQLEN, nb_words))\n",
    "        for i, word in enumerate(test_words):\n",
    "            X_test[0, i, word2index[word]] = 1\n",
    "        pred = model.predict(X_test, verbose=0)[0]\n",
    "        ypred = index2word[np.argmax(pred)]\n",
    "        print(ypred, end=\" \")\n",
    "        # move forward with test_chars + ypred\n",
    "        #test_words = test_words[1:].append(ypred)\n",
    "        del test_words[0]\n",
    "        test_words.append(ypred)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the next character or next word of text is not the only thing you can do with this sort of model. This kind of model has been successfully used to make stock predictions (for more information refer to the article: *Financial Market Time Series Prediction with Recurrent Neural Networks*, by A. Bernal, S. Fok, and R. Pidaparthi, 2012) and generate classical music (for more information refer to the article: *DeepBach: A Steerable Model for Bach Chorales Generation*, by G. Hadjeres and F. Pachet, arXiv:1612.01010, 2016), to name a few interesting applications. Andrej Karpathy covers a few other fun examples, such as generating fake Wikipedia pages, algebraic geometry proofs, and Linux source code in his blog post at: *The Unreasonable Effectiveness of Recurrent Neural Networks at http://karpathy.github.io/2015/05/21/rnn-effectiveness/*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
